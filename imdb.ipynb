{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "import tensorflow as tf\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, train_labels), (test_dataset, test_labels) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_dict = {value: key for key, value in word.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataset[0]:\n",
    "    print(reversed_dict[i], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "x_train = tokenizer.sequences_to_matrix(train_dataset, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(test_dataset, mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(16, activation=\"relu\",input_dim=(1000)))\n",
    "# model.add(Dense(16, activation=\"relu\"))\n",
    "# model.add(Dense(1,activation='sigmoid'))\n",
    "# model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 16, input_length=1000))  # Embedding layer\n",
    "model.add(Flatten())  # Flatten the embedding output\n",
    "model.add(Dense(16, activation='relu'))  # Dense layer 1\n",
    "model.add(Dense(16, activation='relu'))  # Dense layer 2\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels,num_classes=2)\n",
    "test_labels = to_categorical(test_labels,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, train_labels, epochs=10,validation_split=0.2,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/imdb_sementic_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_review = [\"This is an easy to enjoy action packed thrillride, with emotional, satisfying and intense moments\"]\n",
    "print(decoded_review)\n",
    "decoded_review = \"mohsin and rusab\"\n",
    "# Tokenize the new review\n",
    "tokenizer = Tokenizer(num_words=1000)  # Use the same tokenizer configuration as used during training\n",
    "# Assuming you have fitted the tokenizer on training data, otherwise fit on new review text\n",
    "# tokenizer.fit_on_texts(training_data)\n",
    "\n",
    "# Preprocess the new review text\n",
    "new_review_sequence = tokenizer.texts_to_sequences(decoded_review)\n",
    "\n",
    "# Padding sequence to match the model's input shape (assuming the model expects sequences of length 1000)\n",
    "max_sequence_length = 1000  # Adjust according to your model's input shape\n",
    "padded_new_review = pad_sequences(new_review_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions on the preprocessed review\n",
    "predicted_probabilities = model.predict(padded_new_review)\n",
    "print(predicted_probabilities)\n",
    "print(predicted_probabilities.shape)\n",
    "\n",
    "val = predicted_probabilities\n",
    "# Get the class with the highest probability (0 for negative, 1 for positive)\n",
    "predicted_class = int(np.round(predicted_probabilities[0][0]))\n",
    "print(predicted_class)\n",
    "# Convert the predicted class to sentiment label\n",
    "sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "\n",
    "print(f\"The predicted sentiment for the review is: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new work from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding =  \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[],\n",
    "                          dtype=tf.string, trainable=True)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Configure the layers\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data.shuffle(10000, seed=0).batch(512),\n",
    "                    epochs=20,\n",
    "                    validation_data=validation_data.batch(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_index = 9  # Change this index to access different reviews\n",
    "\n",
    "# Load the word index mapping\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "reverse_word_index = {index + 3: word for word, index in word_index.items()}\n",
    "\n",
    "# Decode the review while skipping special tokens (from index 3)\n",
    "decoded_review = ' '.join(reverse_word_index.get(i, '?') for i in train_dataset[review_index] if i >= 3)\n",
    "\n",
    "# Display the decoded review\n",
    "print(\"Decoded Review:\")\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arr = [\"Best movie which i have ever seen.\",\n",
    "       \"The movie was okay but the cast did an horrendous job at displaying emotions, i feel bad for the director but happy for the producers since they made a lot of money\",\n",
    "       \"A good movie with an even better plot. Must watch for people born in the 90's.\",\n",
    "       \"I would rather watch myself eat.\",\n",
    "       \"A terrible performance combined with a terrible script, which is a shame. I would not recommend it at all\",\n",
    "       \"Excellent story! I really enjoyed it.\",\n",
    "       \"I really enjoyed the movie. I would recommend it to anyone.\",\n",
    "       \"My friend recommended me this movie and thats the only reasone I watched it but i hated it.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(my_arr)\n",
    "predicted_labels = []\n",
    "print(pred)\n",
    "for p in pred:\n",
    "    if p > 0.5:\n",
    "        print(\"Positive\")\n",
    "        predicted_labels.append(1)\n",
    "    else:\n",
    "        print(\"Negative\")\n",
    "        predicted_labels.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather n number of reviews to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Change this index to access different reviews\n",
    "decoded_arr = []\n",
    "for review_index in range(0,1000):\n",
    "    \n",
    "    # Load the word index mapping\n",
    "    word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "    reverse_word_index = {index + 3: word for word, index in word_index.items()}\n",
    "\n",
    "    # Decode the review while skipping special tokens (from index 3)\n",
    "    decoded_review = ' '.join(reverse_word_index.get(i, '?') for i in train_dataset[review_index] if i >= 3)\n",
    "\n",
    "    #Display the decoded review\n",
    "    print(\"Decoded Review:\")\n",
    "    print(decoded_review[0:250])\n",
    "    decoded_arr.append(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels.count(1), predicted_labels.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have true labels 'true_labels' and predicted classes 'predicted_classes'\n",
    "# Replace these with your actual true labels and predicted classes from the model\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(train_labels[0:1000], predicted_labels)\n",
    "\n",
    "# Define class names (positive and negative for IMDb sentiment analysis)\n",
    "class_names = ['Negative', 'Positive']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the IMDb dataset\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Access the dataset\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "# Iterate through the dataset to get reviews and labels\n",
    "for review, label in train_data.take(1):\n",
    "    # Decode review from bytes to string\n",
    "    review_text = review.numpy().decode('utf-8')\n",
    "    print(\"Review:\", review_text)\n",
    "    print(\"Label:\", label.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "predicted_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in x_test[0:n]:\n",
    "    image_array = t\n",
    "\n",
    "    # Reshape the image to match the input shape expected by the model\n",
    "    predictions = model.predict(image_array)\n",
    "\n",
    "    # Get the predicted label (digit with highest probability)\n",
    "    predicted_label = predictions.argmax()\n",
    "\n",
    "    #print(predictions)\n",
    "    predicted_labels.append(predicted_label)\n",
    "\n",
    "    #print(f\"Predicted Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
