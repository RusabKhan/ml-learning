{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 18:36:34.249307: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-17 18:36:34.249375: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-17 18:36:34.251573: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-17 18:36:34.264142: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 18:36:35.683072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ubuntu/ml-learning/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import ml_utils.utils as ml_utils\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tensorflow as tf\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Bidirectional,\n",
    "    Dropout,\n",
    "    GlobalMaxPooling1D,\n",
    "    Concatenate,\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    ReLU,\n",
    "    Input,\n",
    "    GRU,\n",
    ")\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.layers import Embedding, Flatten, Dense, Attention\n",
    "import tensorflow_datasets as tfds\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import os\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_train = np.load(\n",
    "#     \"../datasets/COQA-Conversation/COQA_train_final.npz\", allow_pickle=True\n",
    "# )\n",
    "np_test = np.load(\n",
    "    \"../datasets/COQA-Conversation/COQA_test_final.npz\", allow_pickle=True\n",
    ")\n",
    "test_df = pd.DataFrame(np_test[\"data\"], columns=np_test[\"columns\"])\n",
    "# train_df = pd.DataFrame(np_train[\"data\"], columns=np_train[\"columns\"])\n",
    "# np_train = None\n",
    "# np_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = ml_utils.extract_each_value_from_column(test_df)\n",
    "# Create a new DataFrame from the list of new rows\n",
    "test_df = pd.DataFrame(new_rows)\n",
    "\n",
    "new_rows = ml_utils.extract_each_value_from_column(train_df)\n",
    "# Create a new DataFrame from the list of new rows\n",
    "train_df = pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "test_df = test_df.map(ml_utils.remove_stopwords)\n",
    "\n",
    "test_df = test_df.map(ml_utils.stem_text)\n",
    "\n",
    "train_df = train_df.map(ml_utils.remove_stopwords)\n",
    "\n",
    "train_df = train_df.map(ml_utils.stem_text)\n",
    "\n",
    "\n",
    "def tokenize_sentences_by_columns(df, column):\n",
    "    tr_text = df[column]\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(tr_text)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(tr_text)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def pad_sequences_with_zeros(X, maxlen):\n",
    "    X = X.tolist()\n",
    "    return pad_sequences(X, maxlen=maxlen).tolist()\n",
    "\n",
    "\n",
    "test_tfidf = pd.DataFrame()\n",
    "train_tfidf = pd.DataFrame()\n",
    "\n",
    "test_padded = pd.DataFrame()\n",
    "train_padded = pd.DataFrame()\n",
    "\n",
    "for col in test_df.columns:\n",
    "    test_tfidf[col] = tokenize_sentences_by_columns(test_df, col)\n",
    "for col in train_df.columns:\n",
    "    train_tfidf[col] = tokenize_sentences_by_columns(train_df, col)\n",
    "\n",
    "\n",
    "for col in test_tfidf.columns:\n",
    "    test_padded[col] = pad_sequences_with_zeros(test_tfidf[col], 1000)\n",
    "for col in train_tfidf.columns:\n",
    "    train_padded[col] = pad_sequences_with_zeros(train_tfidf[col], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1000, 1)]            0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1000, 1)]            0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 256),                264192    ['input_1[0][0]']             \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 1000, 256),          264192    ['input_2[0][0]',             \n",
      "                              (None, 256),                           'lstm[0][1]',                \n",
      "                              (None, 256)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1000, 1)              257       ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 528641 (2.02 MB)\n",
      "Trainable params: 528641 (2.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_length = None  # Example sequence length\n",
    "vocab_size = 200  # Example vocabulary size\n",
    "\n",
    "# Define encoder input\n",
    "encoder_inputs = Input(shape=(1000, 1))\n",
    "\n",
    "# Define LSTM encoder layer with ndim=2\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder input\n",
    "# Corrected decoder input shape\n",
    "decoder_inputs = Input(shape=(1000, 1))\n",
    "\n",
    "# Define LSTM decoder layer with ndim=2\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Define Dense layer for output\n",
    "decoder_dense = Dense(1, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_input = Input(shape=(1000, 1))\n",
    "question_input = Input(shape=(1000, 1))\n",
    "\n",
    "# Encoder LSTM\n",
    "encoder = LSTM(512, return_sequences=True)\n",
    "encoder_outputs = encoder(story_input)\n",
    "\n",
    "# Attention mechanism\n",
    "# Attention mechanism takes encoder outputs and decoder LSTM hidden states to compute the context vector\n",
    "attention = Attention()\n",
    "context_vector = attention([encoder_outputs, encoder_outputs])  # Use encoder outputs for both keys and values\n",
    "\n",
    "# Decoder LSTM\n",
    "# Decoder LSTM takes the context vector as input and generates output sequences\n",
    "decoder_lstm = LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(context_vector)\n",
    "\n",
    "# Output layer\n",
    "# Dense layer maps decoder outputs to vocabulary size and applies softmax activation\n",
    "decoder_dense = Dense(1, activation=\"softmax\")\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([story_input, question_input], output)\n",
    "\n",
    "# Compile the model\n",
    "# Compile the model using Adam optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 20778.2637 - accuracy: 1.0850e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1710708681.350192   54539 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2199 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 67108864 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 8205s 33s/step - loss: 20778.2637 - accuracy: 1.0850e-04 - val_loss: 21719.5645 - val_accuracy: 1.0550e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1776390940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [x_stories, x_questions],\n",
    "    y_answers,\n",
    "    epochs=1,\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_sequences = np.vstack(train_df[\"question\"].to_numpy())\n",
    "story_sequences = np.vstack(train_df[\"story\"].to_numpy())\n",
    "answers_sequences = np.vstack(train_df[\"answers\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = train_df['story'].head(10000).values\n",
    "questions = train_df['question'].head(10000).values\n",
    "answers = train_df['answers'].head(10000).values\n",
    "\n",
    "x_stories = np.array(stories)\n",
    "x_questions = np.array(questions)\n",
    "y_answers = np.array(answers)\n",
    "\n",
    "stories = None\n",
    "questions = None\n",
    "answers = None\n",
    "\n",
    "x_stories = np.vstack(x_stories)\n",
    "x_questions = np.vstack(x_questions)\n",
    "y_answers = np.vstack(y_answers)\n",
    "\n",
    "train_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = test_df['story'].head(1).values\n",
    "questions = test_df['question'].head(1).values\n",
    "answers = test_df['answers'].head(1).values\n",
    "\n",
    "x_stories = np.array(stories)\n",
    "x_questions = np.array(questions)\n",
    "y_answers = np.array(answers)\n",
    "\n",
    "stories = None\n",
    "questions = None\n",
    "answers = None\n",
    "\n",
    "x_stories = np.vstack(x_stories)\n",
    "x_questions = np.vstack(x_questions)\n",
    "y_answers = np.vstack(y_answers)\n",
    "\n",
    "test_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([x_stories, x_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0, 39]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_texts = []\n",
    "for prediction in predictions:\n",
    "    predicted_text = inverse_transform(prediction)\n",
    "    predicted_texts.append(predicted_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def inverse_transform(tfidf_sequence):\n",
    "    tfidf_matrix = tfidf_sequence.reshape(1, -1)  # Reshape to a 2D matrix\n",
    "\n",
    "    # Inverse transform TF-IDF sequence to text using the TF-IDF vectorizer\n",
    "    text_sequence = TfidfVectorizer.inverse_transform(tfidf_sequence)\n",
    "    \n",
    "    # Remove padding and join tokens into sentences\n",
    "    text_sequence = [token for token in text_sequence if token != 0]  # Assuming 0 represents padding\n",
    "    text = \" \".join(text_sequence)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(prediction, tst)\n",
    "\n",
    "# Identify misclassifications with counts greater than 1\n",
    "misclassified_indices = np.where(cm > 0)\n",
    "\n",
    "# Display the confusion matrix and its shape\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)\n",
    "print(\"Shape of Confusion Matrix:\", cm.shape)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Define colors for the plot\n",
    "cmap = plt.cm.Blues\n",
    "# Set the color for misclassifications\n",
    "#cmap.set_under('yellow')\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0.1)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding axis labels \n",
    "classes = np.unique(tst)\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Adding annotations for misclassifications with counts > 1\n",
    "thresh = cm.max() / 2.0\n",
    "for i, j in zip(*misclassified_indices):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_sequences = story_sequences.reshape(\n",
    "    (story_sequences.shape[0], story_sequences.shape[1], 1)\n",
    ")\n",
    "question_sequences = question_sequences.reshape(\n",
    "    (question_sequences.shape[0], question_sequences.shape[1], 1)\n",
    ")\n",
    "answers_sequences = answers_sequences.reshape(\n",
    "    (answers_sequences.shape[0], answers_sequences.shape[1], 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Example DataFrame\n",
    "\n",
    "# Combine all text data from all columns into a single string\n",
    "all_text = \" \".join(test_df.stack().astype(str))\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = all_text.split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "vocab_size = len(word_counts)\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = train_padded.to_numpy()\n",
    "columns = train_df.columns\n",
    "np.savez_compressed(\n",
    "    \"../datasets/COQA-Conversation/COQA_train_final.npz\",\n",
    "    data=data_array,\n",
    "    columns=columns,\n",
    ")\n",
    "\n",
    "data_array = test_padded.to_numpy()\n",
    "columns = test_df.columns\n",
    "np.savez_compressed(\n",
    "    \"../datasets/COQA-Conversation/COQA_test_final.npz\",\n",
    "    data=data_array,\n",
    "    columns=columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = None\n",
    "train_df = None\n",
    "\n",
    "test_tfidf = None\n",
    "train_tfidf = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
