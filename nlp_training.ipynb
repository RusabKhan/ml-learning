{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tensorflow as tf\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout, GlobalMaxPooling1D, \\\n",
    "Concatenate, BatchNormalization, Conv1D, ReLU, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.calibration import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets and download NLTK lib init functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"datasets/test.txt\", sep=\";\")\n",
    "train = pd.read_csv(\"datasets/train.txt\", sep=\";\")\n",
    "val = pd.read_csv(\"datasets/val.txt\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = f\"{os.getcwd()}/nltk_datasets\"\n",
    "nltk.data.path.append(dir)\n",
    "nltk.download(\"stopwords\", download_dir=dir)\n",
    "nltk.download(\"punkt\", download_dir=dir)\n",
    "nltk.download(\"maxent_ne_chunker\", download_dir=dir)\n",
    "nltk.download(\"words\", download_dir=dir)\n",
    "nltk.download(\"tagsets\", download_dir=dir)\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower()\n",
    "                     not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "\n",
    "def extract_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    ne_chunks = ne_chunk(pos_tags)\n",
    "\n",
    "    entities = []\n",
    "    for chunk in ne_chunks:\n",
    "        if hasattr(chunk, \"label\") and chunk.label():\n",
    "            if chunk.label() == \"NE\":\n",
    "                entities.append(\" \".join([c[0] for c in chunk]))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def create_tfidf_vectorizer(df):\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, use_idf=True)\n",
    "    # Fit and transform the text data in the DataFrame column\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"sentence\"])\n",
    "    # Convert the TF-IDF matrix to a DataFrame for visualization\n",
    "    return tfidf_matrix.toarray()\n",
    "\n",
    "\n",
    "def tokenize_sentences(df):\n",
    "    tr_text = df['sentence']\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(tr_text) \n",
    "                                \n",
    "    sequences = tokenizer.texts_to_sequences(tr_text)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def encode_emotions(emotions):\n",
    "    encoder = LabelEncoder()\n",
    "    return encoder.fit_transform(emotions)\n",
    "\n",
    "def pad_sequences_with_zeros(X, maxlen):\n",
    "    return pad_sequences(X, maxlen=maxlen)\n",
    "\n",
    "def check_frequency_of_words(df):\n",
    "    # Combine all sentences into one string\n",
    "    all_sentences = ' '.join(df['sentence'].tolist())\n",
    "\n",
    "    # Tokenize the combined text into words\n",
    "    words = all_sentences.split()\n",
    "\n",
    "    # Create a Pandas Series to count word frequencies\n",
    "    word_freq = pd.Series(words).value_counts()\n",
    "\n",
    "    # Plot the top 20 most frequent words\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    word_freq.head(20).plot(kind='bar', color='skyblue')\n",
    "    plt.title('Top 20 Most Frequent Words in Sentences')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-process textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"sentence\"] = train[\"sentence\"].apply(remove_stopwords)\n",
    "test[\"sentence\"] = test[\"sentence\"].apply(remove_stopwords)\n",
    "val[\"sentence\"] = val[\"sentence\"].apply(remove_stopwords)\n",
    "\n",
    "train[\"sentence\"] = train[\"sentence\"].apply(stem_text)\n",
    "test[\"sentence\"] = test[\"sentence\"].apply(stem_text)\n",
    "val[\"sentence\"] = val[\"sentence\"].apply(stem_text)\n",
    "\n",
    "# train[\"entities\"] = train[\"sentence\"].apply(extract_entities)\n",
    "# test[\"entities\"] = test[\"sentence\"].apply(extract_entities)\n",
    "# val[\"entities\"] = val[\"sentence\"].apply(extract_entities)\n",
    "\n",
    "train[\"sentence\"] = train[\"sentence\"].str.lower()\n",
    "test[\"sentence\"] = test[\"sentence\"].str.lower()\n",
    "val[\"sentence\"] = val[\"sentence\"].str.lower()\n",
    "\n",
    "train_tfidf = tokenize_sentences(train)\n",
    "test_tfidf = tokenize_sentences(test)\n",
    "val_tfidf = tokenize_sentences(val)\n",
    "\n",
    "train_padded = pad_sequences_with_zeros(train_tfidf, 50)\n",
    "test_padded = pad_sequences_with_zeros(test_tfidf, 50)\n",
    "val_padded = pad_sequences_with_zeros(val_tfidf, 50)\n",
    "\n",
    "train['emotion_en'] = encode_emotions(train['emotion'])\n",
    "test['emotion_en'] = encode_emotions(test['emotion'])\n",
    "val['emotion_en'] = encode_emotions(val['emotion'])\n",
    "# lets do lammentization next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_en_decrypted = train[['emotion','emotion_en']]\n",
    "emotion_en_decrypted = emotion_en_decrypted.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save numpy dataset to reload back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f\"{os.getcwd()}/datasets/preprocessed_data/nlp_training.npz\", train=train_tfidf,test=test_tfidf,val=val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_arr_npz = np.load(f\"{os.getcwd()}/datasets/preprocessed_data/nlp_training.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_arr_npz['train'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(train[\"emotion_en\"])\n",
    "y_test = to_categorical(test[\"emotion_en\"])\n",
    "y_val = to_categorical(val[\"emotion_en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(10000, 64, input_length=50),\n",
    "    LSTM(64,return_sequences=False),\n",
    "    Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(32, activation='relu',kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adamax', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### siamese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_1 = (50,)  # Replace with your input shape\n",
    "input_shape_2 = (50,)  # Replace with your input shape\n",
    "\n",
    "# Define branches for each input\n",
    "input_1 = Input(shape=input_shape_1)\n",
    "input_2 = Input(shape=input_shape_2)\n",
    "\n",
    "# Embedding layer for each input\n",
    "embedding_1 = Embedding(10000, 64, input_length=50)(input_1)\n",
    "embedding_2 = Embedding(10000, 64, input_length=50)(input_2)\n",
    "\n",
    "# Flatten or any necessary layers for each branch\n",
    "flatten_1 = Flatten()(embedding_1)\n",
    "flatten_2 = Flatten()(embedding_2)\n",
    "\n",
    "# Concatenate the branches\n",
    "concatenated = Concatenate()([flatten_1, flatten_2])\n",
    "\n",
    "# Dense layers and final output layer\n",
    "dense_layer = Dense(64, activation='relu')(concatenated)\n",
    "output_layer = Dense(6, activation='softmax')(dense_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamax', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 13s 65ms/step - loss: 2.2835 - accuracy: 0.3369 - val_loss: 1.9933 - val_accuracy: 0.3262\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 1.8772 - accuracy: 0.3390 - val_loss: 1.7838 - val_accuracy: 0.3262\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 1.7288 - accuracy: 0.3528 - val_loss: 1.6743 - val_accuracy: 0.3650\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 1.5792 - accuracy: 0.4577 - val_loss: 1.4848 - val_accuracy: 0.4656\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 5s 50ms/step - loss: 1.3297 - accuracy: 0.5455 - val_loss: 1.2843 - val_accuracy: 0.5512\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 5s 50ms/step - loss: 1.1895 - accuracy: 0.6036 - val_loss: 1.1991 - val_accuracy: 0.5984\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 1.1111 - accuracy: 0.6425 - val_loss: 1.1490 - val_accuracy: 0.6231\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 5s 46ms/step - loss: 1.0485 - accuracy: 0.6655 - val_loss: 1.1383 - val_accuracy: 0.6194\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 0.9945 - accuracy: 0.6836 - val_loss: 1.0872 - val_accuracy: 0.6531\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.9475 - accuracy: 0.7042 - val_loss: 1.0689 - val_accuracy: 0.6625\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 5s 48ms/step - loss: 0.9093 - accuracy: 0.7120 - val_loss: 1.0496 - val_accuracy: 0.6725\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.8667 - accuracy: 0.7254 - val_loss: 1.0182 - val_accuracy: 0.6828\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.8237 - accuracy: 0.7385 - val_loss: 0.9874 - val_accuracy: 0.6888\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.7865 - accuracy: 0.7457 - val_loss: 0.9655 - val_accuracy: 0.6950\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 5s 47ms/step - loss: 0.7458 - accuracy: 0.7555 - val_loss: 0.9333 - val_accuracy: 0.6963\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 5s 52ms/step - loss: 0.7137 - accuracy: 0.7635 - val_loss: 0.9125 - val_accuracy: 0.7025\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 5s 51ms/step - loss: 0.6852 - accuracy: 0.7713 - val_loss: 0.8980 - val_accuracy: 0.7078\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 0.6591 - accuracy: 0.7838 - val_loss: 0.9129 - val_accuracy: 0.7184\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 5s 49ms/step - loss: 0.6290 - accuracy: 0.8048 - val_loss: 0.9059 - val_accuracy: 0.7284\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 5s 48ms/step - loss: 0.6056 - accuracy: 0.8257 - val_loss: 0.9138 - val_accuracy: 0.7322\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_padded, y_train, epochs=20, batch_size=128, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/nlp_training_model_self.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 50)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 50)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_24 (Embedding)    (None, 50, 64)               640000    ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_25 (Embedding)    (None, 50, 64)               640000    ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)        (None, 3200)                 0         ['embedding_24[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)        (None, 3200)                 0         ['embedding_25[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 6400)                 0         ['flatten_13[0][0]',          \n",
      " )                                                                   'flatten_14[0][0]']          \n",
      "                                                                                                  \n",
      " dense_40 (Dense)            (None, 64)                   409664    ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " dense_41 (Dense)            (None, 6)                    390       ['dense_40[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1690054 (6.45 MB)\n",
      "Trainable params: 1690054 (6.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_old = tf.keras.models.load_model(\"models/nlp_training_model_overfit_chatgpt.h5\")\n",
    "model_old.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 3ms/step - loss: 2.9650 - accuracy: 0.2765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.964996814727783, 0.27649998664855957]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "joy         5362\n",
       "sadness     4666\n",
       "anger       2159\n",
       "fear        1937\n",
       "love        1304\n",
       "surprise     572\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.value_counts(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model accuracy with custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('old:', 2, 'joy', '|new:', 2, 'joy')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = [\"i am space\"]\n",
    "df = pd.DataFrame(input_sentence, columns=['sentence'])\n",
    "\n",
    "df[\"sentence\"] = df[\"sentence\"].apply(remove_stopwords)\n",
    "df[\"sentence\"] = df[\"sentence\"].apply(stem_text)\n",
    "\n",
    "input_ = tokenize_sentences(df)\n",
    "input_ = pad_sequences_with_zeros(input_, 50)\n",
    "\n",
    "pred_val = model_old.predict([input_,input_]).argmax()\n",
    "result = emotion_en_decrypted.loc[emotion_en_decrypted['emotion_en'] == pred_val, 'emotion'].values[0]\n",
    "\n",
    "\n",
    "pred_val_model = model.predict(input_).argmax()\n",
    "result_model = emotion_en_decrypted.loc[emotion_en_decrypted['emotion_en'] == pred_val_model, 'emotion'].values[0]\n",
    "\"old:\",pred_val,result,\"|new:\",pred_val_model,result_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_frequency_of_words(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
