{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tensorflow as tf\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"datasets/test.txt\", sep=\";\")\n",
    "train = pd.read_csv(\"datasets/train.txt\", sep=\";\")\n",
    "val = pd.read_csv(\"datasets/val.txt\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = f\"{os.getcwd()}/nltk_datasets\"\n",
    "nltk.data.path.append(dir)\n",
    "nltk.download(\"stopwords\", download_dir=dir)\n",
    "nltk.download(\"punkt\", download_dir=dir)\n",
    "nltk.download(\"maxent_ne_chunker\", download_dir=dir)\n",
    "nltk.download(\"words\", download_dir=dir)\n",
    "nltk.download(\"tagsets\", download_dir=dir)\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "\n",
    "def extract_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    ne_chunks = ne_chunk(pos_tags)\n",
    "\n",
    "    entities = []\n",
    "    for chunk in ne_chunks:\n",
    "        if hasattr(chunk, \"label\") and chunk.label():\n",
    "            if chunk.label() == \"NE\":\n",
    "                entities.append(\" \".join([c[0] for c in chunk]))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def create_tfidf_vectorizer(df):\n",
    "    vectorizer = TfidfVectorizer(max_features=1000,use_idf=True)\n",
    "    # Fit and transform the text data in the DataFrame column\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"sentence\"])\n",
    "    # Convert the TF-IDF matrix to a DataFrame for visualization\n",
    "    return tfidf_matrix.toarray()\n",
    "\n",
    "def check_frequency_of_words(df):\n",
    "    # Combine all sentences into one string\n",
    "    all_sentences = ' '.join(df['sentence'].tolist())\n",
    "\n",
    "    # Tokenize the combined text into words\n",
    "    words = all_sentences.split()\n",
    "\n",
    "    # Create a Pandas Series to count word frequencies\n",
    "    word_freq = pd.Series(words).value_counts()\n",
    "\n",
    "    # Plot the top 20 most frequent words\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    word_freq.head(20).plot(kind='bar', color='skyblue')\n",
    "    plt.title('Top 20 Most Frequent Words in Sentences')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"sentence\"] = train[\"sentence\"].apply(remove_stopwords)\n",
    "test[\"sentence\"] = test[\"sentence\"].apply(remove_stopwords)\n",
    "val[\"sentence\"] = val[\"sentence\"].apply(remove_stopwords)\n",
    "\n",
    "train[\"sentence\"] = train[\"sentence\"].apply(stem_text)\n",
    "test[\"sentence\"] = test[\"sentence\"].apply(stem_text)\n",
    "val[\"sentence\"] = val[\"sentence\"].apply(stem_text)\n",
    "\n",
    "train[\"entities\"] = train[\"sentence\"].apply(extract_entities)\n",
    "test[\"entities\"] = test[\"sentence\"].apply(extract_entities)\n",
    "val[\"entities\"] = val[\"sentence\"].apply(extract_entities)\n",
    "\n",
    "train[\"sentence\"] = train[\"sentence\"].str.lower()\n",
    "test[\"sentence\"] = test[\"sentence\"].str.lower()\n",
    "val[\"sentence\"] = val[\"sentence\"].str.lower()\n",
    "\n",
    "train_tfidf = create_tfidf_vectorizer(train)\n",
    "test_tfidf = create_tfidf_vectorizer(test)\n",
    "val_tfidf = create_tfidf_vectorizer(val)\n",
    "# lets do lammentization next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"emotion\"] = train[\"emotion\"].replace(\n",
    "    [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"], [0, 1, 2, 3, 4, 5]\n",
    ")\n",
    "test[\"emotion\"] = test[\"emotion\"].replace(\n",
    "    [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"], [0, 1, 2, 3, 4, 5]\n",
    ")\n",
    "val[\"emotion\"] = val[\"emotion\"].replace(\n",
    "    [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"], [0, 1, 2, 3, 4, 5]\n",
    ")\n",
    "\n",
    "y_train = to_categorical(train[\"emotion\"])\n",
    "y_test = to_categorical(test[\"emotion\"])\n",
    "y_val = to_categorical(val[\"emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                 \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=6, input_length=1000),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25/25 [==============================] - 104s 4s/step - loss: 1.5808 - accuracy: 0.3373 - val_loss: 1.5753 - val_accuracy: 0.3262\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 115s 5s/step - loss: 1.5770 - accuracy: 0.3373 - val_loss: 1.5737 - val_accuracy: 0.3262\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 104s 4s/step - loss: 1.5767 - accuracy: 0.3373 - val_loss: 1.5732 - val_accuracy: 0.3262\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 105s 4s/step - loss: 1.5760 - accuracy: 0.3373 - val_loss: 1.5743 - val_accuracy: 0.3262\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 116s 5s/step - loss: 1.5764 - accuracy: 0.3373 - val_loss: 1.5731 - val_accuracy: 0.3262\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(train_tfidf,\n",
    "#                     y_train, epochs=20, batch_size=32,\n",
    "#                     validation_data=(val_tfidf, y_val))\n",
    "history = model.fit(train_tfidf,\n",
    "                    y_train, epochs=5, batch_size=512,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rusab1/Work/learntorch/venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"models/nlp_training_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 64)                64064     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64454 (251.77 KB)\n",
      "Trainable params: 64454 (251.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_old = tf.keras.models.load_model(\"models/nlp_training_model.h5\")\n",
    "model_old.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 9s 149ms/step - loss: 1.5591 - accuracy: 0.3475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.559146761894226, 0.3474999964237213]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_frequency_of_words(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
