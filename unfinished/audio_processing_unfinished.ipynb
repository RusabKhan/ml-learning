{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "import tensorflow as tf\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_angry = \"datasets/urdu_lang_audios/Angry/SM1_F10_A010.wav\"\n",
    "file_path_sad = \"datasets/urdu_lang_audios/Sad/SF10_F1_S01.wav\"\n",
    "file_path_happy = \"datasets/urdu_lang_audios/Happy/SF1_F10_H010.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_angry, audio_data_angry = wavfile.read(file_path_angry)\n",
    "sample_rate_sad, audio_data_sad = wavfile.read(file_path_sad)\n",
    "sample_rate_happy, audio_data_happy = wavfile.read(file_path_happy)\n",
    "\n",
    "audio_data_angry_flatten = audio_data_angry.flatten()\n",
    "audio_data_sad_flatten = audio_data_sad.flatten()\n",
    "audio_data_happy_flatten = audio_data_happy.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_angry = pad_sequences(audio_data_angry_flatten, maxlen=132456, dtype='float32', padding='post', truncating='post')\n",
    "audio_data_sad = pad_sequences(audio_data_sad_flatten, maxlen=132456, dtype='float32', padding='post', truncating='post')\n",
    "audio_data_happy = pad_sequences(audio_data_happy_flatten, maxlen=132456, dtype='float32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate_angry, sample_rate_sad, sample_rate_happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.DataFrame([[audio_data_angry, audio_data_sad, audio_data_happy]], columns=[\"angry\", \"sad\", \"happy\"])\n",
    "main = main.T\n",
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_angry = pd.DataFrame(audio_data_angry)\n",
    "df_angry[\"tone\"] = \"angry\"\n",
    "\n",
    "df_sad = pd.DataFrame(audio_data_sad)\n",
    "df_sad[\"tone\"] = \"sad\"\n",
    "\n",
    "df_happy = pd.DataFrame(audio_data_happy)\n",
    "df_happy[\"tone\"] = \"happy\"\n",
    "\n",
    "main_df = pd.concat([df_angry, df_sad, df_happy], axis=0)\n",
    "main_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
